{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "termProject.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYF19Rb29NXQ",
        "outputId": "dffe6fe4-b684-42ec-d723-87cbe10966be"
      },
      "source": [
        "%pip install praw"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2c/15/4bcc44271afce0316c73cd2ed35f951f1363a07d4d5d5440ae5eb2baad78/praw-7.1.0-py3-none-any.whl (152kB)\n",
            "\r\u001b[K     |██▏                             | 10kB 17.0MB/s eta 0:00:01\r\u001b[K     |████▎                           | 20kB 22.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 30kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 40kB 8.9MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 51kB 4.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 71kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 81kB 5.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 102kB 5.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 112kB 5.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 122kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 133kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 143kB 5.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 153kB 5.7MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.17\n",
            "  Downloading https://files.pythonhosted.org/packages/0c/ba/8dd7fa5f0b1c6a8ac62f8f57f7e794160c1f86f31c6d0fb00f582372a3e4/update_checker-0.18.0-py3-none-any.whl\n",
            "Collecting prawcore<2.0,>=1.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1d/40/b741437ce4c7b64f928513817b29c0a615efb66ab5e5e01f66fe92d2d95b/prawcore-1.5.0-py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 26.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.17->praw) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.15.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (2020.11.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.17->praw) (3.0.4)\n",
            "Installing collected packages: update-checker, prawcore, websocket-client, praw\n",
            "Successfully installed praw-7.1.0 prawcore-1.5.0 update-checker-0.18.0 websocket-client-0.57.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2LhhRo5-Xva"
      },
      "source": [
        "# Social Media Data Analysis With Reddit\n",
        "\n",
        "Reddit is a forum-like social media platform where users post their submissions  (text, image, video, gifs, links etc.) under various subreddits. Our purpose is to analyze data collected from Reddit to get insights from the most popular 10 subreddits.\n",
        "\n",
        "We are going to analyze the data in several steps:\n",
        "* First, we will analyze the upvote ratios to determine the controversial posts in `r/popular`. \n",
        "* In the next step, we will analyze the comments of submissions with lowest upvotes ratios.\n",
        "* Then, by counting how many times a subreddit appeared on `r/popular`, we will find the top 10 subreddits. \n",
        "* After that, we will analyze the submissions from each top 10 subreddits and compare their scores and number of comments.\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laakR2OAEYvy"
      },
      "source": [
        "## Preparation\r\n",
        "\r\n",
        "First we make import libraries, get our client id and secret.  \r\n",
        "Here, we use unofficial Reddit API tool 'Python Reddit Access Wrapper (PRAW)'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csgdRhiS-Xva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a933ef4-3f15-455c-9fc0-f4c4bdb65adc"
      },
      "source": [
        "# username: cmpe2051angrybirds\n",
        "# password: angrybirds\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import praw\n",
        "import nltk\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "\n",
        "client_id = 'a1JhjlLlwNSz_Q'\n",
        "secret = 'B7DQQYCVPTh-Wi_SExgdCWrpb6t7TQ'\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0StG-C1-Xva"
      },
      "source": [
        "Next, we instantiate a Reddit class.  \n",
        "Also, we set the period to `\"month\"` for top submissions.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXWfLAnE-Xva"
      },
      "source": [
        "redd = praw.Reddit(\n",
        "    client_id=client_id,\n",
        "    client_secret=secret,\n",
        "    user_agent='AngryBirds_TermProject'\n",
        ")\n",
        "\n",
        "period = 'month'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z7g0yg0a-Xva"
      },
      "source": [
        "## Data Collection\n",
        "\n",
        "Now, we create an empty dataframe with columns:\n",
        "\n",
        "1. Id\n",
        "2. Title\n",
        "3. Author\n",
        "4. Score\n",
        "5. Upvote Ratio (= Upvotes/Total Votes)\n",
        "6. Number of Comments\n",
        "7. Subreddit\n",
        "8. Permalink\n",
        "9. Created (UTC)\n",
        "\n",
        "Next, we collect top 1000 submissions of the last month in the subreddit `r/popular` (*).  \n",
        "Then, we populate the dataframe with the metadata of these posts.  \n",
        "\n",
        "**Caution:** Since, we are fetching up to 1000 submissions, it may take some time.  \n",
        "\n",
        "(*)Brief information about `r/popular`: Although there are countless subreddits on Reddit, there are some special subreddits moderated by the Reddit administrators. `r/popular` and `r/all` are two of them. `r/all` contains submissions with high score from all over Reddit. `r/popular` is almost same as `r/all` except it is filtered down so that it does not contain fake information, explicit etc. content. That is, `r/popular` is a nicer looking version of `r/all`. That's why we are fetching submissions appearing on `r/popular`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "nXMSwFRy-Xva",
        "outputId": "3a254cf2-8124-474c-ffa2-e50c0de002b3"
      },
      "source": [
        "#df = pd.read_pickle('reddit_data.pkl')\n",
        "df = pd.DataFrame({'id':[],'title':[], 'author':[], 'score':[], 'upvote_ratio':[], 'num_comments':[], 'subreddit':[], 'permalink':[], 'created_utc':[]})\n",
        "popular = redd.subreddit('popular')\n",
        "counter = 0\n",
        "i = 0\n",
        "\n",
        "for submission in popular.top(period,limit=1000):\n",
        "    if i >= 100:\n",
        "        print('Fetched {} records'.format(counter))\n",
        "        i = 0\n",
        "\n",
        "    df = df.append({\n",
        "        'id': submission.id,\n",
        "        'title': submission.title,\n",
        "        'author': submission.author,\n",
        "        'score': submission.score,\n",
        "        'upvote_ratio': submission.upvote_ratio,\n",
        "        'num_comments': submission.num_comments,\n",
        "        'subreddit': submission.subreddit.display_name,\n",
        "        'permalink': submission.permalink,\n",
        "        'created_utc': str(datetime.fromtimestamp(int(float(submission.created_utc))))\n",
        "    }, ignore_index=True)\n",
        "    counter = counter + 1\n",
        "    i = i + 1\n",
        "    \n",
        "#df.to_pickle('reddit_data.pkl')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetched 100 records\n",
            "Fetched 200 records\n",
            "Fetched 300 records\n",
            "Fetched 400 records\n",
            "Fetched 500 records\n",
            "Fetched 600 records\n",
            "Fetched 700 records\n",
            "Fetched 800 records\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>score</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>permalink</th>\n",
              "      <th>created_utc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jr5wtk</td>\n",
              "      <td>\"Are you sure you want to go back to the Old V...</td>\n",
              "      <td>lapapinton</td>\n",
              "      <td>194756.0</td>\n",
              "      <td>0.88</td>\n",
              "      <td>4996.0</td>\n",
              "      <td>funny</td>\n",
              "      <td>/r/funny/comments/jr5wtk/are_you_sure_you_want...</td>\n",
              "      <td>2020-11-09 20:43:45</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>k4qide</td>\n",
              "      <td>An anti-gay Hungarian politician has resigned ...</td>\n",
              "      <td>stem12345679</td>\n",
              "      <td>194886.0</td>\n",
              "      <td>0.90</td>\n",
              "      <td>8324.0</td>\n",
              "      <td>worldnews</td>\n",
              "      <td>/r/worldnews/comments/k4qide/an_antigay_hungar...</td>\n",
              "      <td>2020-12-01 18:15:32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>juqm4s</td>\n",
              "      <td>There are massive floods in southeast Mexico r...</td>\n",
              "      <td>m3antar</td>\n",
              "      <td>189949.0</td>\n",
              "      <td>0.93</td>\n",
              "      <td>1808.0</td>\n",
              "      <td>nextfuckinglevel</td>\n",
              "      <td>/r/nextfuckinglevel/comments/juqm4s/there_are_...</td>\n",
              "      <td>2020-11-15 18:25:46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>jv4nra</td>\n",
              "      <td>Every time</td>\n",
              "      <td>ZyleErelis</td>\n",
              "      <td>170256.0</td>\n",
              "      <td>0.95</td>\n",
              "      <td>2279.0</td>\n",
              "      <td>memes</td>\n",
              "      <td>/r/memes/comments/jv4nra/every_time/</td>\n",
              "      <td>2020-11-16 10:33:42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jvawxo</td>\n",
              "      <td>Been gaming with this dude for 15 years. Since...</td>\n",
              "      <td>ghost_knight1121</td>\n",
              "      <td>166668.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3584.0</td>\n",
              "      <td>gaming</td>\n",
              "      <td>/r/gaming/comments/jvawxo/been_gaming_with_thi...</td>\n",
              "      <td>2020-11-16 17:29:50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       id  ...          created_utc\n",
              "0  jr5wtk  ...  2020-11-09 20:43:45\n",
              "1  k4qide  ...  2020-12-01 18:15:32\n",
              "2  juqm4s  ...  2020-11-15 18:25:46\n",
              "3  jv4nra  ...  2020-11-16 10:33:42\n",
              "4  jvawxo  ...  2020-11-16 17:29:50\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyce7lsh-Xvb"
      },
      "source": [
        "## Analyzing the upvote ratios to determine controversial submissions.\n",
        "\n",
        "Let's begin to analyze the data.\n",
        "Before we find the top 10 subreddit, we will look at the upvote ratios to determine whether there is any controversial submission. Because if a submission has low upvote ratio, it means that the content is not very popular."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "yBcH9yn8-Xvb",
        "outputId": "e1ad17a9-3ef6-4acc-ebee-0799e90129a8"
      },
      "source": [
        "df.upvote_ratio.hist()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fc3ca4a4898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkPRN3cU-Xvb"
      },
      "source": [
        "It looks like majority of the submissions have upvote ratio larger than 0.84.  \n",
        "So, we seperate the submission accordingly.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh9spnE--Xvc",
        "outputId": "64888034-d1f6-4ab6-c702-4167e21a53ff"
      },
      "source": [
        "low_ratio = df[df.upvote_ratio < .84]\n",
        "high_ratio = df[df.upvote_ratio >= .84]\n",
        "\n",
        "total = df.shape[0]\n",
        "low_count = low_ratio.shape[0]\n",
        "high_count = high_ratio.shape[0]\n",
        "\n",
        "low_percent = low_count / total * 100\n",
        "high_percent = high_count / total * 100\n",
        "\n",
        "print('Percentage of the submissions with upvote ratio lower than %84: ' + str(low_percent))\n",
        "print('Percentage of the submissions with upvote ratio higher than %84: ' + str(high_percent))\n",
        "print('')\n",
        "print('Subreddit of the submissions with low upvote ratio:')\n",
        "print(low_ratio.subreddit.value_counts())"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Percentage of the submissions with upvote ratio lower than %84: 13.870246085011187\n",
            "Percentage of the submissions with upvote ratio higher than %84: 86.12975391498881\n",
            "\n",
            "Subreddit of the submissions with low upvote ratio:\n",
            "pics                    33\n",
            "WhitePeopleTwitter      11\n",
            "news                     9\n",
            "gaming                   9\n",
            "worldnews                6\n",
            "funny                    5\n",
            "politics                 5\n",
            "PoliticalHumor           5\n",
            "aww                      4\n",
            "facepalm                 4\n",
            "MurderedByWords          3\n",
            "AskReddit                3\n",
            "nextfuckinglevel         3\n",
            "memes                    2\n",
            "gifs                     2\n",
            "insanepeoplefacebook     2\n",
            "PublicFreakout           2\n",
            "BlackPeopleTwitter       2\n",
            "Showerthoughts           2\n",
            "WatchPeopleDieInside     1\n",
            "Cringetopia              1\n",
            "MadeMeSmile              1\n",
            "science                  1\n",
            "iamatotalpieceofshit     1\n",
            "mildlyinfuriating        1\n",
            "Unexpected               1\n",
            "MurderedByAOC            1\n",
            "wholesomememes           1\n",
            "LifeProTips              1\n",
            "awfuleverything          1\n",
            "dankmemes                1\n",
            "Name: subreddit, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0H8jnfoW5Nj"
      },
      "source": [
        "## Analyzing the most controversial 10 submissions\r\n",
        "\r\n",
        "Now, we will sort the rows of `df` by `upvote_ratios` in ascending order and pick the top 10 submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2MaQ-lGlXMf0",
        "outputId": "32ebab11-7c2e-4470-add9-a7ccd008f8ad"
      },
      "source": [
        "controversial = df.sort_values(by='upvote_ratio',ascending=True)[:10]\r\n",
        "controversial"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>author</th>\n",
              "      <th>score</th>\n",
              "      <th>upvote_ratio</th>\n",
              "      <th>num_comments</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>permalink</th>\n",
              "      <th>created_utc</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>ju82uy</td>\n",
              "      <td>Two Navajo women exercising their 1st amendmen...</td>\n",
              "      <td>DontSearchMyBasement</td>\n",
              "      <td>99449.0</td>\n",
              "      <td>0.65</td>\n",
              "      <td>4327.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/ju82uy/two_navajo_women_exerc...</td>\n",
              "      <td>2020-11-14 19:53:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>173</th>\n",
              "      <td>k5cand</td>\n",
              "      <td>Hopefully this will put an end to Trump's elec...</td>\n",
              "      <td>Spicylemon</td>\n",
              "      <td>113870.0</td>\n",
              "      <td>0.66</td>\n",
              "      <td>3890.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/k5cand/hopefully_this_will_pu...</td>\n",
              "      <td>2020-12-02 16:19:23</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>803</th>\n",
              "      <td>k49zt4</td>\n",
              "      <td>Update from previous post. After giving up gam...</td>\n",
              "      <td>Hannibal_Hector</td>\n",
              "      <td>79602.0</td>\n",
              "      <td>0.66</td>\n",
              "      <td>2196.0</td>\n",
              "      <td>gaming</td>\n",
              "      <td>/r/gaming/comments/k49zt4/update_from_previous...</td>\n",
              "      <td>2020-12-01 01:03:33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>485</th>\n",
              "      <td>jrxt5o</td>\n",
              "      <td>Tim Allen just posted this and said this alway...</td>\n",
              "      <td>holyfruits</td>\n",
              "      <td>90136.0</td>\n",
              "      <td>0.68</td>\n",
              "      <td>3261.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/jrxt5o/tim_allen_just_posted_...</td>\n",
              "      <td>2020-11-11 00:51:54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>343</th>\n",
              "      <td>jt0f43</td>\n",
              "      <td>Joe Biden with a beard needs to be a thing</td>\n",
              "      <td>idea4granted</td>\n",
              "      <td>98975.0</td>\n",
              "      <td>0.68</td>\n",
              "      <td>5193.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/jt0f43/joe_biden_with_a_beard...</td>\n",
              "      <td>2020-11-12 18:46:04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>jvfkmc</td>\n",
              "      <td>Joe Biden’s 1967 Corvette which was gifted to ...</td>\n",
              "      <td>TetraCGT</td>\n",
              "      <td>99503.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>5330.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/jvfkmc/joe_bidens_1967_corvet...</td>\n",
              "      <td>2020-11-16 21:26:40</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>456</th>\n",
              "      <td>k1ncfz</td>\n",
              "      <td>40th birthday, my wife is sick, my newest cons...</td>\n",
              "      <td>scraffe</td>\n",
              "      <td>91944.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>2736.0</td>\n",
              "      <td>gaming</td>\n",
              "      <td>/r/gaming/comments/k1ncfz/40th_birthday_my_wif...</td>\n",
              "      <td>2020-11-26 21:03:10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>744</th>\n",
              "      <td>js5lwk</td>\n",
              "      <td>Time Is Running Out</td>\n",
              "      <td>dittidot</td>\n",
              "      <td>81152.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/js5lwk/time_is_running_out/</td>\n",
              "      <td>2020-11-11 10:10:25</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>757</th>\n",
              "      <td>jtixgo</td>\n",
              "      <td>In front of the White House now</td>\n",
              "      <td>ohnoh18</td>\n",
              "      <td>80717.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>5818.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/jtixgo/in_front_of_the_white_...</td>\n",
              "      <td>2020-11-13 16:33:31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>375</th>\n",
              "      <td>jxelim</td>\n",
              "      <td>A photo of the presidents and their dog.</td>\n",
              "      <td>D0NW0N</td>\n",
              "      <td>96350.0</td>\n",
              "      <td>0.70</td>\n",
              "      <td>2010.0</td>\n",
              "      <td>pics</td>\n",
              "      <td>/r/pics/comments/jxelim/a_photo_of_the_preside...</td>\n",
              "      <td>2020-11-20 00:12:08</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         id  ...          created_utc\n",
              "333  ju82uy  ...  2020-11-14 19:53:23\n",
              "173  k5cand  ...  2020-12-02 16:19:23\n",
              "803  k49zt4  ...  2020-12-01 01:03:33\n",
              "485  jrxt5o  ...  2020-11-11 00:51:54\n",
              "343  jt0f43  ...  2020-11-12 18:46:04\n",
              "330  jvfkmc  ...  2020-11-16 21:26:40\n",
              "456  k1ncfz  ...  2020-11-26 21:03:10\n",
              "744  js5lwk  ...  2020-11-11 10:10:25\n",
              "757  jtixgo  ...  2020-11-13 16:33:31\n",
              "375  jxelim  ...  2020-11-20 00:12:08\n",
              "\n",
              "[10 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLF84Su7ZMej"
      },
      "source": [
        "Next, we put all the title in `controversial` into `text`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRjO4JHNYmWa"
      },
      "source": [
        "text = ' '.join(controversial.title.tolist())"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swGVeVv3ZS-t"
      },
      "source": [
        "Next, we put all the comments in each submission in `controversial` into `text`.\r\n",
        "\r\n",
        "**Caution**: We are fetching a lot of data. It may take some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGSH0OT6ZcXi",
        "outputId": "dfbc24b5-674b-413a-b8e4-275543becf2a"
      },
      "source": [
        "for _id in controversial.id.tolist():\r\n",
        "  submission = redd.submission(_id)\r\n",
        "  submission.comment_sort = 'best' # or we can use 'controversial' to get the comments with the most downvotes.\r\n",
        "  submission.comment_limit = 100\r\n",
        "  submission.comments.replace_more(limit=0) # this is a trick so that `comments.list()` below returns list of objects of the `comment` class, not the `MoreComments` class\r\n",
        "  text = text + ' '.join([comm.body for comm in submission.comments.list()])\r\n",
        "  print('Fetched comments for {}'.format(_id))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetched comments for ju82uy\n",
            "Fetched comments for k5cand\n",
            "Fetched comments for k49zt4\n",
            "Fetched comments for jrxt5o\n",
            "Fetched comments for jt0f43\n",
            "Fetched comments for jvfkmc\n",
            "Fetched comments for k1ncfz\n",
            "Fetched comments for js5lwk\n",
            "Fetched comments for jtixgo\n",
            "Fetched comments for jxelim\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw6fq1WTcoWf"
      },
      "source": [
        "Next, we filter `text` and calculate frequency distribution. Then, pick the most frequent 20 words.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oEGgJI7c6_R"
      },
      "source": [
        "from nltk.corpus import stopwords\r\n",
        "\r\n",
        "my_stopwords = [\r\n",
        "                'like', 'would', 'one', 'people', 'get', 'time', 'even', 'got',\r\n",
        "                'never', 'know', 'really', 'think', 'first', 'good', 'also', \r\n",
        "                'still', 'its'\r\n",
        "]\r\n",
        "\r\n",
        "def clean(word):\r\n",
        "  if word[-1] == '.':\r\n",
        "    word = word[:-1]\r\n",
        "\r\n",
        "  if word[-2:] == \"'s\":\r\n",
        "    word = word[:-2]\r\n",
        "\r\n",
        "  if word[-2:] == '’s':\r\n",
        "    word = word[:-2]\r\n",
        "\r\n",
        "  return word\r\n",
        "\r\n",
        "def filter(word):\r\n",
        "  if word in my_stopwords:\r\n",
        "    return False\r\n",
        "\r\n",
        "  if word in stopwords.words('english'):\r\n",
        "    return False\r\n",
        "\r\n",
        "  if word.startswith('http://'):\r\n",
        "    return False\r\n",
        "\r\n",
        "  if word.startswith('https://'):\r\n",
        "    return False\r\n",
        "\r\n",
        "  return True\r\n",
        "\r\n",
        "words = text.lower().split()\r\n",
        "words = [clean(word) for word in words]\r\n",
        "clean = [word for word in words if filter(word)]\r\n",
        "freq = nltk.FreqDist(clean)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTbdtTKEfUmT",
        "outputId": "22e92fc2-c694-4a75-947d-b8160e5f7a4b"
      },
      "source": [
        "freq.pprint(maxlen=20)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FreqDist({'trump': 93, 'biden': 40, 'way': 38, 'years': 34, 'president': 34, \"i'm\": 32, 'right': 31, 'white': 31, 'much': 31, 'love': 31, 'say': 31, 'see': 30, 'could': 30, 'going': 30, 'look': 29, 'ever': 29, 'said': 28, 'something': 28, 'fucking': 28, 'shit': 28, ...})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjHIfSLj-Xvc"
      },
      "source": [
        "## Findind the top 10 subreddits\r\n",
        "\r\n",
        "Next, we count how many times subreddits have appeared on the top 1000 submissions from the last month and pick the top ten subreddits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l3s5Fsbn-Xvc"
      },
      "source": [
        "best_subreddits = df.subreddit.value_counts()\n",
        "best_subreddits[:10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "plOJAMLK-Xvc"
      },
      "source": [
        "Now, in order to analyze the top 10 individual subreddits, we create a dictionary where each key is the name of the subreddit and the corresponding value is a dataframe populated by the metadata of the top 50 submissions of the last month.\n",
        "\n",
        "**Caution**: We are fetching a lot of data. It may take some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sk1O6c5Y-Xvc"
      },
      "source": [
        "top_ten_subs = best_subreddits.index.values.tolist()[:10]\n",
        "sub_data = dict()\n",
        "for sub in top_ten_subs:\n",
        "    sub_data[sub] = pd.read_pickle('{}.pkl'.format(sub))\n",
        "    \n",
        "    #sub_data[sub] = pd.DataFrame({'id':[],'title':[], 'author':[], 'score':[], 'upvote_ratio':[], 'num_comments':[], 'permalink':[], 'created_utc':[]})\n",
        "    \n",
        "    #for submission in redd.subreddit(sub).top(period, limit=50):\n",
        "    #    sub_data[sub] = sub_data[sub].append({\n",
        "    #        'id': submission.id,\n",
        "    #        'title': submission.title,\n",
        "    #        'author': submission.author,\n",
        "    #        'score': submission.score,\n",
        "    #        'upvote_ratio': submission.upvote_ratio,\n",
        "    #        'num_comments': submission.num_comments,\n",
        "    #        'permalink': submission.permalink,\n",
        "    #        'created_utc': str(datetime.fromtimestamp(int(float(submission.created_utc))))\n",
        "    #    }, ignore_index=True)\n",
        "        \n",
        "    print('Finished collecting data for {}'.format(sub))\n",
        "        \n",
        "    #sub_data[sub].to_pickle('{}.pkl'.format(sub))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6rJp6RXGE-vi"
      },
      "source": [
        "# Example\n",
        "#sub_data['memes'].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCVTZNPp-Xvc"
      },
      "source": [
        "### Statistical analysis of the score in the top 10 subreddits\r\n",
        "\r\n",
        "We want to compare the popularity between the top 10 subreddits. We do this by analyzing the score of the submissions.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwRQPftUiS94"
      },
      "source": [
        "We look up the maximum, minimum, mean, median and standard deviation of the scores of the submissions in the previos dataframes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30C4P6ZU-Xvc"
      },
      "source": [
        "max_scores = [sub_data[sub].score.max() for sub in top_ten_subs]\n",
        "mean_scores = [sub_data[sub].score.mean() for sub in top_ten_subs]\n",
        "median_scores = [sub_data[sub].score.median() for sub in top_ten_subs]\n",
        "min_scores = [sub_data[sub].score.min() for sub in top_ten_subs]\n",
        "std_scores = [sub_data[sub].score.std() for sub in top_ten_subs]\n",
        "\n",
        "top_ten_scores = pd.DataFrame({\n",
        "    'Min': min_scores,\n",
        "    'Max': max_scores,\n",
        "    'Median': median_scores,\n",
        "    'Mean': mean_scores,\n",
        "    'Std Deviation': std_scores,\n",
        "    'Std Dev / Mean': np.array(std_scores) / np.array(mean_scores)\n",
        "}, index=top_ten_subs)\n",
        "\n",
        "top_ten_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3569P0Xy-Xvc"
      },
      "source": [
        "Here, we plot the scores of the individual submissions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "470DMX6d-Xvc"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "for sub in top_ten_subs:\n",
        "    plt.plot(sub_data[sub].score.sort_values(ascending=True,ignore_index=True), label=sub)\n",
        "\n",
        "plt.xlabel('Submission Number')   \n",
        "plt.ylabel('Upvotes')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3_w7fvO-Xvc"
      },
      "source": [
        "### Statistical analysis of the number of comments in the top 10 subreddits.\r\n",
        "\r\n",
        "Now, we want to find where the most discussion happens. We do this by comparing the number of comments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWYOBvBg-Xvc"
      },
      "source": [
        "max_num_comments = [sub_data[sub].num_comments.max() for sub in top_ten_subs]\n",
        "mean_num_comments = [sub_data[sub].num_comments.mean() for sub in top_ten_subs]\n",
        "median_num_comments = [sub_data[sub].num_comments.median() for sub in top_ten_subs]\n",
        "min_num_comments = [sub_data[sub].num_comments.min() for sub in top_ten_subs]\n",
        "std_num_comments = [sub_data[sub].num_comments.std() for sub in top_ten_subs]\n",
        "\n",
        "top_ten_num_comments = pd.DataFrame({\n",
        "    'Min': min_num_comments,\n",
        "    'Max': max_num_comments,\n",
        "    'Median': median_num_comments,\n",
        "    'Mean': mean_num_comments,\n",
        "    'Standard Deviation': std_num_comments,\n",
        "    'Std Dev / Mean': np.array(std_num_comments) / np.array(mean_num_comments) \n",
        "}, index=top_ten_subs)\n",
        "\n",
        "top_ten_num_comments"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iojuVZeV-Xvc"
      },
      "source": [
        "Here, plotting the number of comment for each individual submission."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPcXFWb1-Xvc"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "for sub in top_ten_subs:\n",
        "    plt.plot(sub_data[sub].num_comments.sort_values(ascending=True,ignore_index=True), label=sub)\n",
        "\n",
        "plt.xlabel('Submission Number')   \n",
        "plt.ylabel('Number of Comments')\n",
        "plt.grid()\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}